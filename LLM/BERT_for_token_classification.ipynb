{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "Finetune DistilBERT on the WNUT 17 dataset to detect new entities.\n",
        "Use your finetuned model for inference."
      ],
      "metadata": {
        "id": "7ihvhTXGicgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by loading the WNUT 17 dataset from the ðŸ¤— Datasets library:"
      ],
      "metadata": {
        "id": "McY5KXwmijhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wnut_17\")"
      ],
      "metadata": {
        "id": "6OOxQR7zTRSP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icGTAM6oTRU0",
        "outputId": "7efee637-79f6-4bed-e6ce-84acf4e4b0d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 3394\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1009\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 1287\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMeZuctmTRXO",
        "outputId": "fa23547f-bf3a-430f-fb18-86a13cfbef67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '8',\n",
              " 'tokens': ['Friday', 'Night', 'Eats', 'http://twitpic.com/2pdvtr'],\n",
              " 'ner_tags': [0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
        "label_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRq0vyGNTRZ2",
        "outputId": "7b565061-5c3d-449d-a09f-2d82401b8eb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-corporation',\n",
              " 'I-corporation',\n",
              " 'B-creative-work',\n",
              " 'I-creative-work',\n",
              " 'B-group',\n",
              " 'I-group',\n",
              " 'B-location',\n",
              " 'I-location',\n",
              " 'B-person',\n",
              " 'I-person',\n",
              " 'B-product',\n",
              " 'I-product']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The letter that prefixes each ner_tag indicates the token position of the entity:\n",
        "\n",
        "\n",
        "*   B- indicates the beginning of an entity\n",
        "\n",
        "*   I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n",
        "\n",
        "\n",
        "*  0 indicates the token doesn't correspond to any entity\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HKhK4zG-jVVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "rpMSIC_YTRcH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[\"train\"][0]\n",
        "tokenized_input = tokenizer(example[\"tokens\"],is_split_into_words=True)\n",
        "print(tokenized_input[\"input_ids\"])\n",
        "tokens =  tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0XYVDOxTRe2",
        "outputId": "daf0aa32-e724-4762-fda5-3bbd9e6d487c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNbK722jTRhB",
        "outputId": "d4e059b7-c6fe-409d-f2d7-ca7fe108014e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54zRnaOFmXRw",
        "outputId": "79c707da-8ec5-4ae1-f434-538188c0aef2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this adds some special tokens [CLS] and [SEP] and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords."
      ],
      "metadata": {
        "id": "7ZqovmXomlkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Mapping all tokens to their corresponding word with the word_ids method.\n",
        "2.   Assigning the label -100 to the special tokens [CLS] and [SEP] so they're ignored by the PyTorch loss function.\n",
        "3.Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3uiim6MOpNoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "9d6opRHsTRju"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets map function. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:"
      ],
      "metadata": {
        "id": "kYFs9tDDqibM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "3ltpgZKITRmO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Ugp9SNKATRo3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "Sk1uhyrlTRrt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "cFVvJF_OTRzl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you start training your model, create a map of the expected ids to their labels with id2label and label2id:"
      ],
      "metadata": {
        "id": "HYVsmGKIr22Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"O\",\n",
        "    1: \"B-corporation\",\n",
        "    2: \"I-corporation\",\n",
        "    3: \"B-creative-work\",\n",
        "    4: \"I-creative-work\",\n",
        "    5: \"B-group\",\n",
        "    6: \"I-group\",\n",
        "    7: \"B-location\",\n",
        "    8: \"I-location\",\n",
        "    9: \"B-person\",\n",
        "    10: \"I-person\",\n",
        "    11: \"B-product\",\n",
        "    12: \"I-product\",\n",
        "}\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-corporation\": 1,\n",
        "    \"I-corporation\": 2,\n",
        "    \"B-creative-work\": 3,\n",
        "    \"I-creative-work\": 4,\n",
        "    \"B-group\": 5,\n",
        "    \"I-group\": 6,\n",
        "    \"B-location\": 7,\n",
        "    \"I-location\": 8,\n",
        "    \"B-person\": 9,\n",
        "    \"I-person\": 10,\n",
        "    \"B-product\": 11,\n",
        "    \"I-product\": 12,\n",
        "}"
      ],
      "metadata": {
        "id": "RfgvQkykTR2H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CJb6fmPTR4e",
        "outputId": "55d76be1-d222-45bb-d98d-6e707c807ea5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_wnut_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Wm9By4wPTR9O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "UfE8v2TWtN15",
        "outputId": "18b7e4f8-7021-4f88-f26b-421f2992411e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [81/81 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.68152117729187,\n",
              " 'eval_precision': 0.005356186395286556,\n",
              " 'eval_recall': 0.07414272474513438,\n",
              " 'eval_f1': 0.009990633780830472,\n",
              " 'eval_accuracy': 0.013423966482835278,\n",
              " 'eval_runtime': 5.7743,\n",
              " 'eval_samples_per_second': 222.884,\n",
              " 'eval_steps_per_second': 14.028}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "yTMsIc9HtS3G",
        "outputId": "5c9d318c-7ac3-4aa6-b049-9adbd88b4e4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1065' max='1065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1065/1065 02:17, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.278258</td>\n",
              "      <td>0.624746</td>\n",
              "      <td>0.285449</td>\n",
              "      <td>0.391858</td>\n",
              "      <td>0.940618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.256084</td>\n",
              "      <td>0.576052</td>\n",
              "      <td>0.329935</td>\n",
              "      <td>0.419564</td>\n",
              "      <td>0.943226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>0.270109</td>\n",
              "      <td>0.540613</td>\n",
              "      <td>0.376274</td>\n",
              "      <td>0.443716</td>\n",
              "      <td>0.946219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>0.287609</td>\n",
              "      <td>0.578561</td>\n",
              "      <td>0.365153</td>\n",
              "      <td>0.447727</td>\n",
              "      <td>0.946518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.053700</td>\n",
              "      <td>0.292569</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.378128</td>\n",
              "      <td>0.455611</td>\n",
              "      <td>0.947116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='162' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [81/81 00:28]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1065, training_loss=0.11445152233464058, metrics={'train_runtime': 137.8751, 'train_samples_per_second': 123.082, 'train_steps_per_second': 7.724, 'total_flos': 229344858861480.0, 'train_loss': 0.11445152233464058, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "e7herB2TtYJw",
        "outputId": "e2585026-0fac-4b1d-b2e4-9b0c8bcdb36b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [81/81 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.2560841143131256,\n",
              " 'eval_precision': 0.5760517799352751,\n",
              " 'eval_recall': 0.329935125115848,\n",
              " 'eval_f1': 0.4195639363582793,\n",
              " 'eval_accuracy': 0.9432260271044419,\n",
              " 'eval_runtime': 3.6608,\n",
              " 'eval_samples_per_second': 351.561,\n",
              " 'eval_steps_per_second': 22.126,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""
      ],
      "metadata": {
        "id": "ztcsWT2stbS9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Ensure the model and data are on the same device (either GPU or CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "# Replace 'model' and 'tokenizer' with the actual model and tokenizer you are using\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "VV8c7QEg3JTO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"token-classification\", model, tokenizer=tokenizer,device=0 if device == \"cuda\" else -1)"
      ],
      "metadata": {
        "id": "-RhV5B3wtbVu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86YMcQotbX5",
        "outputId": "1401d202-3ead-4498-9f7c-80282f41693a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'B-location',\n",
              "  'score': 0.29789796,\n",
              "  'index': 1,\n",
              "  'word': 'the',\n",
              "  'start': 0,\n",
              "  'end': 3},\n",
              " {'entity': 'B-location',\n",
              "  'score': 0.6854044,\n",
              "  'index': 2,\n",
              "  'word': 'golden',\n",
              "  'start': 4,\n",
              "  'end': 10},\n",
              " {'entity': 'I-location',\n",
              "  'score': 0.68216825,\n",
              "  'index': 3,\n",
              "  'word': 'state',\n",
              "  'start': 11,\n",
              "  'end': 16},\n",
              " {'entity': 'I-group',\n",
              "  'score': 0.28247955,\n",
              "  'index': 4,\n",
              "  'word': 'warriors',\n",
              "  'start': 17,\n",
              "  'end': 25},\n",
              " {'entity': 'B-location',\n",
              "  'score': 0.28391644,\n",
              "  'index': 7,\n",
              "  'word': 'american',\n",
              "  'start': 33,\n",
              "  'end': 41},\n",
              " {'entity': 'B-location',\n",
              "  'score': 0.8134942,\n",
              "  'index': 13,\n",
              "  'word': 'san',\n",
              "  'start': 80,\n",
              "  'end': 83},\n",
              " {'entity': 'I-location',\n",
              "  'score': 0.53298944,\n",
              "  'index': 14,\n",
              "  'word': 'francisco',\n",
              "  'start': 84,\n",
              "  'end': 93},\n",
              " {'entity': 'I-group',\n",
              "  'score': 0.1500106,\n",
              "  'index': 15,\n",
              "  'word': '.',\n",
              "  'start': 93,\n",
              "  'end': 94}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tVGblLE9tbcv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iI4tsL2D7sMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}